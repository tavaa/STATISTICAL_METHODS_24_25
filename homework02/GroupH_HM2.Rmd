---
title: "GroupH_HM2"
author: "Simonutti, Younes Pour Langaroudi, Billo, Tavano, Vicig"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
date: "2024-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## FSDS - Chapter 4

### Ex 4.2

*For a sequence of observations of a binary random variable, you observe the geometric random variable (Section 2.2.2) outcome of the first success on observation number $y$ = 3. Find and plot the likelihood function.*

**Solution**

The probability mass function (p.m.f) of the **Geometric Distribution** is given by:


$$ f_Y(y; p) = (1 - p)^{y - 1} p, \quad y  = 1, 2, 3, \dots , $$ 
Substituting $y = 3$ in the formula we obtain:

$$ f(3;p) = (1 - p)^{3 - 1}p  = (1- p)^{2}p $$ 
Compute the likelihood function:

 $$ L(\theta) = L(p)= \prod_{i=1}^n f(y_i; p) = \prod_{i=1}^n [(1-p)^{y_i-1}p] =(1 - p )^{\sum_{i=1}^n (y_i - 1)} p $$

$$L(p) = p(1 - p)^2$$

We can graphically represent the likelihood function:

```{r}
# Define the likelihood function for y = 3
likelihood_function <- function(p) {
  return((1 - p)^2 * p)
}

# Generate a sequence of p values
p_values <- seq(0, 1, length.out = 100)

# Calculate the likelihood for each p value
likelihood_values <- likelihood_function(p_values)
max_pi <- p_values[which.max(likelihood_values)]

# Plot the likelihood function
plot(p_values, likelihood_values-max(likelihood_values), type = "l", col = "blue", 
     main = "Likelihood Function for Geometric Distribution", 
     xlab = "Probability of success (p)", ylab = "likelihood-max(likelihood)")

```

The graph shows that with $y = 3$, meaning that it was necessary to have 3 trials before a success, the most likely parameter to have generated this data is $p$ = `r max_pi `

### Ex 4.4

*For the Students data file and corresponding population, find the ML estimate of the population proportion believing in life after death. Construct a Wald 95% confidence interval, using its formula (4.8). Interpret.*

(4.8) $$ \hat \pi = z_{\alpha/2} \sqrt{\frac{\hat \pi (1-\hat \pi)}{n}} $$

**Solution**

```{r}
url <- "https://stat4ds.rwth-aachen.de/data/Students.dat"

students <- read.table(url, header = TRUE)

head(students)
```

```{r}
n = nrow(students)
life_after_death = students$life["life" = 1]

waldInterval = function(x, n, conf.level = 0.95){
   p <- x/n
   sd <- sqrt(p*((1-p)/n))
   z <- qnorm(c( (1 - conf.level)/2, 1 - (1-conf.level)/2)) 
   #returns the value of thresholds at which conf.level has to be cut at. for 95% CI, 
   #this is -1.96 and +1.96
   ci <- p + z*sd
   return(ci)
}
waldInterval(life_after_death, n)
```
The interval is extremely small and includes 0: most probably the portion of students that believe in life after death is not significantly different from 0. However, it must be noted that the interval also contains values smaller than 0, which obviously cannot be taken by the parameter of a proportion. For this reason, other types of tests may be more suitable in this situation.


### Ex 4.38

*For independent observations $y_1, . . . , y_n$ having the geometric distribution (2.1):* 

*(a) Find a sufficient statistic for $\pi$.* 
*(b) Derive the ML estimator of $\pi$.*

**Solution**

a)

The probability mass function (p.m.f) of the **Geometric Distribution** is given by:

$$ f_Y(y; p) = (1 - p)^{y - 1} p, \quad y  = 1, 2, 3, \dots , $$ 
For n independent observations $y_1, \dots , y_n$, the joint likelihood function is:

 $$ L(p; y_1, \dots, y_n)= \prod_{i=1}^n P(Y_i = y_i) = \prod_{i=1}^n [(1-p)^{y_i-1}p] =(1 - p )^{\sum_{i=1}^n (y_i - 1)} p^n $$

The sufficient statistic, derived from the formula is given by the exponential part ${\sum_{i=1}^n (y_i - 1)}$ . Thus, the likelihood depends on the data only through ${\sum_{i=1}^n y_i}$.

In conclusion, by the Factorization Theorem, $s(y) = {\sum_{i=1}^n y_i}$ is a sufficient statistic for $p$.

b)

In order to find ML for $p$ analytically, we need to compute the log-likelihood function:

$$ l(p) = log(L(p; y_1, \dots, y_n)) = \sum_i^n y_i\ log(1-p)+n \log(p)$$
Now, we differentiate $l(p)$ with respect to $p$ and set it to 0:

$$ \frac{\partial \ l(p) }{\partial \ p} = \frac{\sum_i^n y_i}{1-p} - \frac{n}{p} = 0$$
$$ p \sum_i^n y_i  = n(1-p) => p = \frac {n}{\sum_i^n y_i}$$
In conclusion, The Maximum Likelihood Estimate (MLE) for $p$ is:

$$ \hat{p} = \frac{n}{\sum_{i=1}^n y_i} $$

### Ex 4.44

*Refer to the previous two exercises. Consider the selling prices (in thousands of dollars) in the Houses data file mentioned in Exercise 4.31.* 
*(a) Fit the normal distribution to the data by finding the ML estimates of $\mu$ and $\sigma$ for that distribution.* 

*(b) Fit the log-normal distribution to the data by finding the ML estimates of its parameters.* 

*(c) Find and compare the ML estimates of the mean and standard deviation of selling price for the two distributions.* 

*(d) Superimpose the fitted normal and log-normal distributions on a histogram of the data. Which distribution seems to be more appropriate for summarizing the selling prices?*

**Solution**

```{r}
url_houses = "https://stat4ds.rwth-aachen.de/data/Houses.dat"

houses <- read.table(url_houses, header = TRUE)

head(houses)
```
a)

```{r}
likelihood_normal <- function(mu, sigma2, data) {
  n <- length(data)
  constant <- 1 / sqrt(2 * pi * sigma2)
  exponent <- exp(-((data - mu)^2) / (2 * sigma2))
  likelihood <- prod(constant * exponent)
  return(-likelihood)
}

mu_hat = mean(houses$price)
s2_hat = var(houses$price)

ML_sigma = optim(
  par = s2_hat,  # Initial guess for sigma^2
  fn = likelihood_normal,
  mu = mu_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

ML_mu = optim(
  par = mu_hat,  # Initial guess for sigma^2
  fn = likelihood_normal,
  sigma = s2_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

comparison <- data.frame(
  Quantity = c("Mean", "Variance"),
  MLE_Function = c(ML_mu$par, ML_sigma$par),
  Built_in_Function = c(mu_hat, s2_hat)
)

comparison
```
b)

```{r}
log_likelihood_normal <- function(mu, sigma2, data) {
  #browser()
  n <- length(data)
  log_constant <- -n / 2 * log(2 * pi * sigma2)  # Logarithm of the constant term
  sum_squared <- sum((data - mu)^2)             # Sum of squared deviations
  log_exponent <- -sum_squared / (2 * sigma2)   # Logarithm of the exponent
  log_likelihood <- log_constant + log_exponent
  return(-log_likelihood)  # Return the negative log-likelihood for optimization
}

log_likelihood_normal(mu_hat, s2_hat, houses$price)

Log_ML_sigma = optim(
  par = s2_hat,  # Initial guess for sigma^2
  fn = log_likelihood_normal,
  mu = mu_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

Log_ML_mu = optim(
  par = mu_hat,  # Initial guess for sigma^2
  fn = log_likelihood_normal,
  sigma = s2_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)
```

c)
```{r}
comparison2 <- data.frame(
  Quantity = c("Mean", "Variance"),
  MLE_Function = c(Log_ML_mu$par, Log_ML_sigma$par),
  Built_in_Function = c(mu_hat, s2_hat)
)

comparison2
```
```{r}
# Load required libraries
library(ggplot2)

# Example data: Replace with your data
data <- houses$price

# Fit the Normal distribution
mu_normal <- mean(data)       # Mean
sigma_normal <- sd(data)      # Standard deviation

# Fit the Log-Normal distribution
log_data <- log(data)         # Log-transform the data
mu_log <- mean(log_data)      # Mean of log-transformed data
sigma_log <- sd(log_data)     # Std dev of log-transformed data
```

d)
```{r}
# Histogram of the data
ggplot(data = data.frame(price = data), aes(x = price)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
  
  # Add Normal PDF
  stat_function(
    fun = dnorm,
    args = list(mean = mu_normal, sd = sigma_normal),
    color = "red",
    size = 1.2,
    aes(linetype = "Normal")
  ) +
  
  # Add Log-Normal PDF
  stat_function(
    fun = function(x) dlnorm(x, meanlog = mu_log, sdlog = sigma_log),
    color = "green",
    size = 1.2,
    aes(linetype = "Log-Normal")
  ) +
  
  # Labels and Legend
  labs(
    title = "Histogram with Fitted Normal and Log-Normal Distributions",
    x = "Selling Prices",
    y = "Density",
    linetype = "Distribution"
  ) +
  theme_minimal()

```
It appears that the log-normal distribution fits the data better.


### Ex 4.54

*Consider $n$ independent observations from an exponential pdf $f(y;\lambda) = \lambda e^{− \lambda y}$ for $y$ $\ge$ 0, with parameter $\lambda$ > 0 for which $E(Y) = \frac{1}{\lambda}$.* $\\$

*(a) Find the sufficient statistic for estimating $\lambda$*. $\\$
*(b) Find the maximum likelihood estimator of $\lambda$ and of $E(Y)$. $\\$*
*(c) One can show that $2 \lambda (\sum_i{Y_i})$ has a chi-squared distribution with $df = 2n$. Explain why*
*$2 \lambda (\sum_i{Y_i})$ is a pivotal quantity, and use it to derive a 95% confidence interval for $\lambda$.*

**Solution**

a)

To find the sufficient statistic for $\lambda$, we use the factorization theorem which states that a statistic T(y) is sufficient for a parameter $\lambda$ if the joint p.d.f. can be factored in two parts: one part depends on the data y only through T(y) and $\lambda$ and the other depends on the data y but not on $\lambda$.

The exponential p.d.f is:

$$ f(y; \lambda) = \lambda e^{-\lambda y}, \quad y \ge 0 $$
with $E[Y] = \frac{1}{\lambda}$ 

For $n$ independent observations $y_1, y_2, \dots, y_n$ the joint p.d.f is:

$$ f(y_1, y_2, \dots, y_n; \lambda) = \prod_{i = 1}^n \lambda e^{-\lambda \ y_i} $$
$$ f(y_1, y_2, \dots, y_n; \lambda) =  \lambda^n e^{-\lambda \sum_{i = 1}^n y_{i}} $$
Now we can identify the sufficient statistic:

$$ g(T(y), \lambda) \ h(y) $$
We can identify from the previous formula:

$T(y) = \sum_{i = 1}^n y_{i}$, $g(T(y), \lambda) = \lambda^n e^{-\lambda \sum_{i = 1}^n y_{i}}$ and $h(y) = 1$.

Thus, the sufficient statistic for $\lambda$ is $\sum_{i = 1}^n y_i$.

b)

The log-likelihood function is:

$$ l(\lambda) = log(L(\lambda)) = log(f(y;\lambda)) = n \ log \ \lambda \ - \lambda \sum_{i = 1}^n y_i $$
To find the MLE of $\lambda$ we have to differentiate $l(\lambda)$ w.r.t $\lambda$ and set it equal to 0.

$$ \frac{\partial \ l(\lambda) }{\partial \ \lambda} = \frac{n}{y} - \sum_{i = 1}^n y_i = 0$$
Solving for $\lambda$ we obtain $\hat \lambda = \frac{n}{\sum_{i = 1}^n y_i}$. This is our MLE.


As $E[Y] = \frac{1}{\lambda}$, it follows that
$$
 \hat E[Y] = \frac{1}{\hat \lambda} =  \frac{\sum_{i = 1}^n y_i}{n}
$$
Which corresponds to the sample mean.

c)

A pivotal quantity is a function of the sample data and the parameter of interest that has a known probability distribution, independent of the parameter's actual value. In this case:

$$
Q =  2 \lambda (\sum_i{Y_i})
$$
It is pivotal because it is a function of the sample data $\sum_i{Y_i}$ and the parameter $\lambda$. Its distribution does not depend on $\lambda$ and it follows a chi-squared distribution with 2 degrees of freedom.

$$ Q \sim X_{2n}^2 $$
This independence from $\lambda$ makes Q a pivotal quantity. 

To find the C.I. we set this equality:

$$ P (q_{\alpha/2} \le Q \le q_{1-\alpha/2} ) = 1 - \alpha$$
From the definition of Q:

$$ P (q_{\alpha/2} \le 2 \lambda \sum_i{Y_i} \le q_{1-\alpha/2} ) = 1 - \alpha$$

We can therefore write
$$
P(\chi_{2n, \frac{\alpha}{2}}^2 < 2 \lambda \sum{Y_i} < \chi_{2n, 1 - \frac{\alpha}{2}}^2 )
$$
In conclusion, with some simple algebra, the 95% C.I. for $\lambda$ is:

$$
\lambda \in [\frac{\chi_{2n, \frac{\alpha}{2}}^2}{2 \sum y_i}; \frac{\chi_{2n, 1 - \frac{\alpha}{2}}^2}{2 \sum y_i} ]
$$

## FSDS - Chapter 5

### Ex 5.68

*Explain why the confidence interval based on the Wald test of $H_0: \theta = \theta_0$ is symmetric around $\hat \theta$ (i.e., having center exactly equal to $\hat \theta$. This is not true for the confidence intervals based on the likelihood-ratio and score tests.) Explain why such symmetry can be problematic when $\theta$ and $\hat \theta$ are near a boundary, using the example of a population proportion that is very close to 0 or 1 and a sample proportion that may well equal 0 or 1.*

**Solution**

The Wald Confidence Interval is symmetric around $\hat \theta$ because it is build from the normal approximation:

$$ C.I. = \hat \theta \pm z_{\alpha/2}SE(\hat \theta) $$
The symmetry is supported from the assumption that the sampling distribution of $\hat \theta$ is approximately normal and centered in $\hat \theta$.

On the other hand, confidence intervals based on the likelihood-ratio test and score test are not necessarily symmetric. These methods are derived from the curvature of the likelihood function or deviations under the null hypothesis, which may lead to asymmetric intervals that depends on the properties of the data.

The symmetry of Wald type intervals becomes problematic when $\hat \theta$ is near a boundary (i.e. a proportion close to 0 or 1). If we imagine that the result of a C.I. sets the lower bound below 0, that would make no sense since a proportion cannot be negative. Another problem could arise when the upper bound is greater than 1, because the proportions are included in $[0,1]$.

These issues arise because the Wald C.I. does not account for the constraints of the parameter space. 

## FSDS - Chapter 6

### Ex 6.12

*For the UN data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.*

**Solution**

```{r}
UN_url <- "https://stat4ds.rwth-aachen.de/data/UN.dat"

UN <- read.table(UN_url, header = TRUE)

# UN$Nation = as.factor(UN$Nation)
# excluding the nation column because of multicollinearity
fit = lm(Internet ~ . - Nation, data = UN)
summary(fit)

```


```{r}

fit_gdp = lm(Internet ~ GDP, data = UN)
summary(fit_gdp)

```
Including all the other variables just marginally increases the R squared.

```{r}
library(ggcorrplot)
corr_matrix = cor(UN[, -1])
ggcorrplot(corr_matrix)
```
```{r}
# Find excessive correlations (excluding the diagonal)
excessive_corr <- which(abs(corr_matrix) > 0.70 & upper.tri(corr_matrix), arr.ind = TRUE)

# Print the pairs of predictors with excessive correlation
for (i in 1:nrow(excessive_corr)) {
  row_name <- rownames(corr_matrix)[excessive_corr[i, 1]]
  col_name <- colnames(corr_matrix)[excessive_corr[i, 2]]
  corr_value <- corr_matrix[excessive_corr[i, 1], excessive_corr[i, 2]]
  
  print(paste(row_name, "and", col_name, "are correlated with value:", round(corr_value, 2)))
}

```
```{r}
# how many variables are correlated with gdp and by how much?
corr_matrix["GDP", ]
```

In the complete model the effect of the main variable GDP is dispersed in lots of other less relevant variables.
This however does not mean that the simpler model with just GDP is equal or better than the more complex model
```{r}
c(AIC(fit), AIC(fit_gdp))
anova(fit, fit_gdp, test = "Chisq")

```
In fact, both ANOVA and AIC tests confirm that they are indeed significantly different in their prediction quality, with the simpler model performing worse.


### Ex 6.14

*The data set Crabs2 at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, $SpermTotal$, is the log of the total number of sperm in an ejaculate. It has $\bar y$ = 19.3 and $s$ = 2.0. The two explanatory variables used in the R output are the horseshoe crab’s $carapace width$ (CW, mean 18.6 $cm$, standard deviation 3.0 $cm$), which is a measure of its size, and $color$ (1 = dark, 2 = medium, 3 = light), which is a measure of adult age, darker ones being older.*

*(a) Using the results shown, write the prediction equation and interpret the parameter estimates.*
*(b) Explain the differences in what is tested with the F statistic*
  *(i) for the overall model,   (ii) for the factor(Color) effect*
  *(iii) for the interaction term. Interpret each*

**Solution**

```{r}
crabs2_url <- "https://stat4ds.rwth-aachen.de/data/Crabs2.dat"

crabs2 = read.table(crabs2_url, header = TRUE)
head(crabs2)
crabs2$Color = as.factor(crabs2$Color)
```
a)

```{r}
fit_crab = lm(SpermTotal ~ CW + Color, data = crabs2)
summary(fit_crab)
```

**Prediction Equation:**

$$ SpermTotal =11.3589620+0.3911538×CW+0.8081103×Color2+1.1487937×Color3 $$
**Interpretation of the estimates:**

  - 1 extra centimeter of carapace width contributes to approximately $(1 - e^{0.3911538}) 100 \%$ to sperm production.
  - A crab having color 2 contributes $(1 - e^{0.8081103}) 100 \%$ to sperm production
  - A crab having color 3 contributes $(1 - e^{1.1487937}) 100 \%$ to sperm production
  
b) 

The F-statistic in an analysis of variance (ANOVA) or regression context is used to test different hypotheses depending on what aspect of the model is being examined. 

```{r}
anova(lm(SpermTotal ~ CW + factor(Color) + CW:factor(Color), data=crabs2))
```
(i) the test on the overall model checks whether it is significantly different from the Null model; more specifically if **at least one** predictor is significantly different from 0 ( this is the case, as both CW and colour(s) are statistically significant).

(ii) Regarding the color effect, it checks whether there is a significant difference between different color groups.

(iii) Regarding the interaction effect, it tests whether there is a significant difference between color groups when taking into account the combined effect of Carapace weight and Color. In this case, the P value is > 0.2 so we can't reject H0 (there is no interaction effect).

### Ex 6.30

*When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat \beta_1$ in the bivariate linear model are also then multiplied by $c$. Thus, show that r = $\hat \beta_1(\frac{s_x}{s_y})$ does not depend on the units of measurement.*

**Solution**

Given the bivariate linear model $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$ and a constant $c$ s.t. $y^*_i = cy_i$, we have that: 

$$s_{y^*} = \sum_{i=1}^n (y_i^*-\bar{y}^*) = \sum_{i=1}^n (cy_i-c\bar{y}) = c\sum_{i=1}^n (y_i-\bar{y}) = cs_y $$
and:

$$
\hat{\beta}_1^* = \frac{s_{xy^*}}{s_x^2} =  \frac{\sum_{i=1}^n (y_i^*-\bar{y}^*)(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2} = \frac{\sum_{i=1}^n c(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2} = c\hat{\beta}_1
$$ 
Finally, given $u_1$ and $u_2$ constants that represent different units of measurement respectively for $x$ and $y$, s.t. 
$x^* = u_1x; \quad  y^* = u_2y$

$$
r^* = \hat{\beta}_1^*(s_{x^*}/s_{y^*}) = \frac{s_{x^*y^*}{s_{x^*}}}{s_{x^*}^2s_{y^*}} =  \frac{\sum_{i=1}^n (y_i^*-\bar{y}^*)(x_i^*-\bar{x}^*)\sum_{i=1}^n(x_i^*-\bar{x}^*)}{\sum_{i=1}^n (x_i^*-\bar{x}^*)^2\sum_{i=1}^n (y_i^*-\bar{y}^*)} = 
$$
$$
= \frac{\sum_{i=1}^n u_1u_2(y_i-\bar{y})(x_i-\bar{x})\sum_{i=1}^nu_1(x_i-\bar{x})}{\sum_{i=1}^n u_1^2(x_i-\bar{x})^2\sum_{i=1}^n u_2(y_i-\bar{y})} = \hat{\beta}_1(s_x/s_y) = r
$$
Therefore we can conclude that $r$ does not depend on the units of measurement.

### Ex 6.42

*You can fit the quadratic equation E(Y) = $\beta_0 + \beta_1 x + \beta_2 x^2$ by fitting a multiple regression model with $x_1 = x$ and $x_2 = x^2$.*

*(a) Simulate 100 independent observations from the model $Y = 40.0−5.0x+0.5x^2 + \epsilon$, where X has a uniform distribution over [0, 10] and $\epsilon  ∼ N (0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship*
*(b) Find the correlation between x and y and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.*

**Solution**

a)

```{r}
set.seed(42) # For reproducibility

n = 100
X = runif(n, min = 0, max = 10)
epsilon = rnorm(n, 0, 1)
Y = 40.0 - 5.0*X+0.5* X^2 + epsilon
plot(X, Y)
data = data.frame(y = Y, x = X)
fit_quadratic <- lm(y ~ poly(x, 2, raw = TRUE), data = data)

#Install & load ggplot2                 
library("ggplot2") 
  
# Create basic ggplot 
# and Add regression line 
ggp <- ggplot(data, aes(x, y)) +            
  geom_point() 
ggp = ggp +                                      
  geom_smooth(method = "lm", 
              formula = y ~ poly(x, 2, raw = TRUE)) 
ggp
```
The regression line with the quadratic term fits the data very well even though it ignores the other terms. The quadratic term prevails as X grows, resulting in the model being an overall good fit even if some points are spread around the regression line and not exactly on it.

b)

First of all, we print the summary of the model and the correlation between X and Y:

```{r}
print(summary(fit_quadratic))
```

```{r}
correlation <- cor(X, Y)
print(correlation)
```
The correlation is so low (`r correlation`) despite the large $R^2$ value of the model because Pearson's correlation coefficient can't capture non-linear relationships.

### Ex 6.52

*F statistics have alternate expressions in terms of $R^2$ values.*

*(a) Show that for testing $H_0 : \beta_1 = ... = \beta_p$ = 0,*
$$ F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} $$ 
*is equivalent to:*

$$ F = \frac{R^2/p}{(1-R^2)/[n-(p+1)]} $$ 
*Explain why larger values of $R^2$ yield larger values of $F$.*

*(b) Show that for comparing nested linear models,*

$$ F = \frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} \quad = \quad \frac{R_1^2-R_0^2/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]} $$

**Solution**

a)

F-statistic for testing $H_0 : \beta_1 = ... = \beta_p$ = 0 is given by:

$$ F = \frac{Explained \ Variance \ per \ Predictor}{Unexplained \ Variance \ per \ Residual  \ Degree \  of \ Freedom}$$
This is expressed as:

$$ F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} $$ 
We can relate $R^2$ to $TSS$ and $SSE$:

$$ R^2 = \frac{TSS-SSE}{TSS} \quad => \quad TSS - SSE = R^2 \ TSS$$
The proportion of unexplained variance is: $1-R^2= \frac{SSE}{TSS}$.

And we can express SSE as: $SSE=(1-R^2) \ TSS$.

We can now substitute this results into $F$-statistics formula:

$$ F = \frac{(R^2 \ TSS) \ / p}{[(1-R^2) \ TSS] \ / [n-(p+1)]}$$
Simplifying the TSS term in the numerator and denominator, we obtain:

$$ F = \frac{R^2 \ / p}{(1-R^2) \  \ / [n-(p+1)]}$$
Rearranging:

$$ F = \frac{R^2}{p} \ \frac{n - (p+1)}{1 - R^2}$$
The result we have obtained is equal to:

$$ F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} $$ 
A larger proportion of explained variance (higher $R^2$) results in a more significant F-value, indicating stronger evidence against $H_0$.

b)

From the previous point:

$$ R^2 = 1 - \frac{SSE}{TSS} $$
For the reduced (0 index) and the full model (1 index):

$$ R^2_0 = 1 - \frac{SSE_0}{TSS} \quad and \quad R^2_1 = 1 - \frac{SSE_1}{TSS} $$
The reduction in SSE when moving from the reduced model to the full model can be expressed in terms of $R^2$:

$$ SSE_0 - SSE_1 = (R_1^2 - R_0^2) \ TSS $$
We can substitute into the numerator of the F-statistics formula:

$$ \frac{SSE_0 - SSE_1}{p_1-p_0} = \frac{(R_1^2 - R_0^2) \ TSS}{p_1-p_0} $$
Where $p_1$ and $p_0$ are the number of predictors in the full model and reduced models, respectively.

For the full model, the denominator becomes:

$$ \frac{SSE_1}{n-(p_1+1)} = \frac{(1-R_1^2)\ TSS}{n-(p_1+1)} $$
We can combine numerator and denominator and obtain:

$$ F = \frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} \quad = \quad \frac{R_1^2-R_0^2/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]} $$

## FSDS - Chapter 7

### Ex 7.4

*Analogously to the previous exercise, randomly sample 30 X observations from a uniform in the interval (-4,4) and conditional on X = $x$, 30 normal observations with E(Y) = $3.5x^3$ − $20x^2$ + $0.5x$ + 20 and $\sigma$ = 30. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for E(Y) = $0.5x^3$ − $20x^2$ + $0.5x$ + 20 (same $\sigma$) several times. What do you observe? Which model would you suggest now?*

**Solution**

```{r}
n = 30
sigma = 30
X = runif(n, min = -4, max = 4)
mu_y = 3.5*X^3 - 20*X^2 + 0.5*X + 20

E_Y = sapply(mu_y, function(mu) rnorm(n, mean = mu, sd = sigma))
Y = colMeans(E_Y)

par(mfrow = c(1, 2))

plot(X, Y, pch = 16, col = rgb(0, 0, 1, 0.5), 
     xlab = "X", ylab = "Y", main = "Scatter Plot of X vs Y")

# Fit polynomial models
glm1 = glm(Y ~ X, family = gaussian())               # Linear
glm2 = glm(Y ~ poly(X, 2), family = gaussian())      # Quadratic
glm3 = glm(Y ~ poly(X, 3), family = gaussian())      # Cubic
glm4 = glm(Y ~ poly(X, 4), family = gaussian())      # Quartic
```
```{r}
# Predictions for each model
pred1 = predict(glm1, newdata = data.frame(X))
pred2 = predict(glm2, newdata = data.frame(X))
pred3 = predict(glm3, newdata = data.frame(X))
pred4 = predict(glm4, newdata = data.frame(X))

# Create a data frame for predictions
pred_data <- data.frame(
  X = X,
  Y = c(pred1, pred2, pred3, pred4),
  Model = factor(rep(c("Linear", "Quadratic", "Cubic", "Quartic"), each = length(X)))
)

data = data.frame(X, Y)
# Step 2: Create the base plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "blue", alpha = 0.5) +  # Scatter plot of X vs Y
  geom_line(data = pred_data, aes(x = X, y = Y, color = Model, linetype = Model), size = 1) +
  scale_color_manual(values = c("red", "green", "blue", "purple")) +  # Manual color mapping
  labs(
    title = "Polynomial Fits",
    x = "X",
    y = "Y",
    color = "Model",
    linetype = "Model"
  ) +
  theme_minimal()  # Use a clean theme

```
As we can see, the quartic model (purple line) seems to fit the data best, whereas the quadratic and linear models appear incapable of capturing the complexity of the relationship. Choosing a model such as the quartic one, or of higher degree, would just introduce unnecessary complexity in explaining the relationship, and significantly imbalance the bias-variance tradeoff.

```{r}
n = 30
sigma = 30
X = runif(n, min = -4, max = 4)
mu_y = 0.5*X^3 - 20*X^2 + 0.5*X + 20

E_Y = sapply(mu_y, function(mu) rnorm(n, mean = mu, sd = sigma))
Y = colMeans(E_Y)

# Fit polynomial models
glm1 = glm(Y ~ X, family = gaussian())               # Linear
glm2 = glm(Y ~ poly(X, 2), family = gaussian())      # Quadratic
glm3 = glm(Y ~ poly(X, 3), family = gaussian())      # Cubic
glm4 = glm(Y ~ poly(X, 4), family = gaussian())      # Quartic

# Predictions for each model
pred1 = predict(glm1, newdata = data.frame(X))
pred2 = predict(glm2, newdata = data.frame(X))
pred3 = predict(glm3, newdata = data.frame(X))
pred4 = predict(glm4, newdata = data.frame(X))

# Create a data frame for predictions
pred_data <- data.frame(
  X = X,
  Y = c(pred1, pred2, pred3, pred4),
  Model = factor(rep(c("Linear", "Quadratic", "Cubic", "Quartic"), each = length(X)))
)

data = data.frame(X, Y)
# Step 2: Create the base plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "blue", alpha = 0.5) +  # Scatter plot of X vs Y
  geom_line(data = pred_data, aes(x = X, y = Y, color = Model, linetype = Model), size = 1) +
  scale_color_manual(values = c("red", "green", "blue", "purple")) +  # Manual color mapping
  labs(
    title = "Polynomial Fits",
    x = "X",
    y = "Y",
    color = "Model",
    linetype = "Model"
  ) +
  theme_minimal()  # Use a clean theme

```
Now all models but the linear one fit the data almost perfectly: the low coefficient on the cubic term has made it almost irrelevant in the interval, compared to the quadratic coefficient. 

### Ex 7.20

*In the Crabs data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).*

*(a) Fit a main-effects ( no interaction terms) logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.*
*(b) Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.*
*(c) Use AIC to determine which models seem most sensible among the models with (i) interaction, (ii) main effects, (iii) weight as the sole predictor, (iv) color as the sole predictor, and (v) the null model.*

**Solution**


```{r}
crabs_url <- "https://stat4ds.rwth-aachen.de/data/Crabs.dat"

crabs = read.table(crabs_url, header = TRUE)

head(crabs)

```
a)

Using the glm function, we build a logistic regression model and we obtain a summary.

```{r}
fit = glm(y ~ weight + color, family = binomial(link = "logit"), data = crabs)
summary(fit)
```
Significance test for the color effect:

```{r}
anova_model <- anova(fit, test = "Chisq")
cat("Significativity test for color:\n")
print(anova_model)
```
Although the effect is less strong than in weight, the color effect is still significant (p < 0.05).
This suggests that color contributes to improved model fit and explains some of the variability in response.

Building the confidence interval:

```{r}
CI_weight = fit$coefficients[2] + c(-1,1)*qnorm(0.975)*(summary(fit)$coefficients[, "Std. Error"][2])
CI_weight

# check
CI_weight_function <- confint(fit, param="weight")
CI_weight_function
# the intervals match!
```
b)

Fitting the model that permits interaction between color as a factor and weight in their effects.

```{r}
fit1 = glm(y ~ weight + factor(color) + weight:factor(color), family = binomial(link = "logit"), data = crabs)
summary(fit1)
```
```{r}
c(AIC(fit), AIC(fit1))
```

```{r}
anova(fit1, fit, test = "LRT")
```
The extended model scores a slightly lower deviance, but it doesn't significantly improve with the AIC. Also with LRT, the difference between the 2 models is not significant even at the 10% level. 
We thus prefer the smaller and simpler model.

c)

We tested various models that seem the most sensible. In particular, we tested the model with interaction, the main model (fit), the model with weights as the only predictor, color as the only predictor and the null model.

```{r}
# Fit the models
model_interaction <- glm(y ~ weight * color, family = binomial, data = crabs)
model_main <- glm(y ~ weight + color, family = binomial, data = crabs)
model_weight <- glm(y ~ weight, family = binomial, data = crabs)
model_color <- glm(y ~ color, family = binomial, data = crabs)
model_null <- glm(y ~ 1, family = binomial, data = crabs)
```

**Comparing the AIC**

```{r}
aic_values <- AIC(model_interaction, model_main, model_weight, model_color, model_null)
print(aic_values)
```
The main effects models scores the lowest AIC criteria, and is therefore the best model.

**Anova**

```{r}
anova(model_null, model_color, model_weight, model_main, model_interaction, test = "Chisq")
```
The best model is the main effects one (4), as it represents a good balance between simplicity and explanatory power. The addition of both predictors significantly reduces residual deviance compared with the simpler models, indicating that both weight and color help to explain variability in the response variable. However, the inclusion of an interaction term between the two predictors does not make a significant improvement, suggesting that their effects are mainly additive. Therefore, the model with only main effects effectively balances complexity and statistical significance, respecting the principle of parsimony.

### Ex 7.26

*A headline in The $Gainesville$ $Sun$ (Feb. 17, 2014) proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.*

**Solution**

**Poisson model**

```{r}
shark_attacks <- c(33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23)

poisson_model <- glm(shark_attacks ~ 1, family = poisson(link = "log"))
summary(poisson_model)
```

```{r}
#dispersion
dispersion <- sum(residuals(poisson_model, type = "deviance")^2) / df.residual(poisson_model)
dispersion

residual_deviance <- poisson_model$deviance
residual_df <- poisson_model$df.residual
p_value <- pchisq(residual_deviance, residual_df, lower.tail = FALSE)

cat("Residual Deviance:", residual_deviance, "\n")
cat("Residual DF:", residual_df, "\n")
cat("P-value :", p_value, "\n")

```
The residual deviance is significantly different from the expected deviance. Poisson's model may not be suitable. Moreover, the value obtained from pchisq is very low, indicating that the data at hand cannot come from a Null Poisson model.

**Negative Binomial**

```{r}
library(MASS)

model_nb <- glm.nb(shark_attacks ~ 1)

summary(model_nb)

```
```{r}
pchisq(model_nb$deviance, poisson_model$df.residual, lower.tail = F)
```
In the case of the negative binomial, from the observation of the p-value, we cannot reject the null hypothesis that the counts at hand originate from such a distribution. 

```{r}
#aic comparison
aic_comparison <- AIC(poisson_model, model_nb)
cat("Comparison AIC:\n")
print(aic_comparison)
```

AIC of the negative binomial is lower than the Poisson, thus indicating a better fit.


